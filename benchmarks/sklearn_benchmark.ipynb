{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benckmarking <a name=\"head\"></a>\n",
    "\n",
    "In the following we will train some models and check the performance of AtoML against sklearn.\n",
    "\n",
    "## Table of Contents\n",
    "[(Back to top)](#head)\n",
    "\n",
    "-   [Requirements](#requirements)\n",
    "-   [Setup](#setup)\n",
    "-   [1D-Model](#1d-model)\n",
    "-   [Real Data](#real-data)\n",
    "-   [Feature Dimensionality](#feature-dimensionality)\n",
    "\n",
    "## Requirements <a name=\"requirements\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "-   [AtoML](https://gitlab.com/atoml/AtoML)\n",
    "-   [ASE](https://wiki.fysik.dtu.dk/ase/)\n",
    "-   [numpy](http://www.numpy.org/)\n",
    "-   [scikit-learn](http://scikit-learn.org/stable/)\n",
    "\n",
    "## Setup <a name=\"setup\"></a>\n",
    "[(Back to top)](#head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from ase.ga.data import DataConnection\n",
    "from ase.io import write\n",
    "\n",
    "from atoml.api.ase_data_setup import get_unique, get_train\n",
    "from atoml.fingerprint.setup import FeatureGenerator\n",
    "from atoml.regression import GaussianProcess\n",
    "from atoml.regression.cost_function import get_error\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, RBF\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-Model <a name=\"1d-model\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "To start with, we consider a simple 1D model. This is perhaps not an ideal benchmarking case as we would tend to be interested in a far greater number of dimensions within the feature space. But using this kind of toy data we can easily generate varying numbers of data points. In particular, it is worth noting that in the following 10,000 test data points are generated, there are only a small number of training points.\n",
    "\n",
    "### scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# Generate sample data\n",
    "X = 15 * rng.rand(200, 1)\n",
    "y = np.sin(X).ravel()\n",
    "y += 3 * (0.5 - rng.rand(X.shape[0]))  # add noise\n",
    "\n",
    "gp_kernel = 1. * RBF(length_scale=0.5) + WhiteKernel(1e-1)\n",
    "gpr = GaussianProcessRegressor(kernel=gp_kernel)\n",
    "stime = time.time()\n",
    "gpr.fit(X, y)\n",
    "print(\"Time for sklearn fitting: %.3f\" % (time.time() - stime))\n",
    "\n",
    "X_plot = np.linspace(0, 20, 10000)[:, None]\n",
    "stime = time.time()\n",
    "y_gpr = gpr.predict(X_plot, return_std=False)\n",
    "print(\"Time for sklearn prediction: %.3f\" % (time.time() - stime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using sklearn to train and test a GP, both procedures are very fast. It is then possible to see what hyperparameters were used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpr.kernel_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AtoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdict = {\n",
    "    'k1': {'type': 'gaussian', 'width': [0.5], 'scaling': 1.},\n",
    "}\n",
    "\n",
    "stime = time.time()\n",
    "gp = GaussianProcess(\n",
    "    kernel_dict=kdict, regularization=1e-1, train_fp=X, train_target=y,\n",
    "    optimize_hyperparameters=True, scale_data=False)\n",
    "print(\"Time for atoml fitting: %.3f\" % (time.time() - stime))\n",
    "\n",
    "stime = time.time()\n",
    "y_atoml = gp.predict(test_fp=X_plot, uncertainty=True)\n",
    "print(\"Time for atoml prediction: %.3f\" % (time.time() - stime))\n",
    "y_atoml = y_atoml['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AtoML performs similarly quickly when training this simple model. However, it is a lot slower at making predictions on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gp.kernel_dict, 'regularization:', gp.regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "lw = 2\n",
    "plt.scatter(X, y, c='k', label='data')\n",
    "plt.plot(X_plot, np.sin(X_plot), color='navy', lw=lw, label='True')\n",
    "plt.plot(X_plot, y_gpr, color='turquoise', lw=lw,\n",
    "         label='sklearn')\n",
    "plt.plot(X_plot, y_atoml, color='darkorange', lw=lw,\n",
    "         label='atoml')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(-4, 4)\n",
    "plt.title('scikit-learn vs AtoML')\n",
    "plt.legend(loc=\"best\",  scatterpoints=1, prop={'size': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some small differences in the predicted function generated by sklearn and AtoML. Though there is generally pretty good agreement, with differences likely due to the way hyperparameters are optimized by the two codes.\n",
    "\n",
    "## Real Data <a name=\"real-data\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "We can run the comparisons with a more realistic data set. In the following, we will import approximately 1300 nanoparticle atoms objects and generate feature vectors approcimately 150 in length. 800 data points will be used to train the model and the remaining data will be used to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect ase atoms database.\n",
    "gadb = DataConnection('../data/gadb.db')\n",
    "\n",
    "# Get all relaxed candidates from the db file.\n",
    "all_cand = gadb.get_all_relaxed_candidates(use_extinct=False)\n",
    "\n",
    "testset = get_unique(atoms=all_cand, size=272, key='raw_score')\n",
    "\n",
    "trainset = get_train(atoms=all_cand, size=800, taken=testset['taken'],\n",
    "                     key='raw_score')\n",
    "\n",
    "generator = FeatureGenerator(atom_types=[78, 79], nprocs=1)\n",
    "\n",
    "train_features = generator.return_vec(trainset['atoms'], [generator.eigenspectrum_vec])\n",
    "train_targets = trainset['target']\n",
    "\n",
    "test_features = generator.return_vec(testset['atoms'], [generator.eigenspectrum_vec])\n",
    "test_targets = testset['target']\n",
    "\n",
    "vec_names = generator.return_names([generator.eigenspectrum_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_kernel = 1. * RBF(length_scale=1.) + WhiteKernel(1e-1)\n",
    "gpr = GaussianProcessRegressor(kernel=gp_kernel)\n",
    "stime = time.time()\n",
    "scalar = StandardScaler()\n",
    "train_features = scalar.fit_transform(train_features)\n",
    "test_features = scalar.transform(test_features)\n",
    "gpr.fit(train_features, train_targets)\n",
    "print(\"Time for sklearn fitting: %.3f\" % (time.time() - stime))\n",
    "\n",
    "stime = time.time()\n",
    "y_gpr = gpr.predict(test_features, return_std=False)\n",
    "print(\"Time for sklearn prediction: %.3f\" % (time.time() - stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpr.kernel_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AtoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdict = {\n",
    "    'k1': {'type': 'gaussian', 'width': 1., 'scaling': 1., 'dimension': 'single'},\n",
    "}\n",
    "\n",
    "stime = time.time()\n",
    "gp = GaussianProcess(\n",
    "    kernel_dict=kdict, regularization=1e-1, train_fp=train_features, train_target=train_targets,\n",
    "    optimize_hyperparameters=True, scale_data=True)\n",
    "print(\"Time for atoml fitting: %.3f\" % (time.time() - stime))\n",
    "\n",
    "stime = time.time()\n",
    "y_atoml = gp.predict(test_fp=test_features, uncertainty=True)\n",
    "print(\"Time for atoml prediction: %.3f\" % (time.time() - stime))\n",
    "y_atoml = y_atoml['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gp.kernel_dict, 'regularization:', gp.regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(test_targets, test_targets, color='navy', label='True')\n",
    "plt.scatter(test_targets, y_gpr, color='turquoise',\n",
    "         label='sklearn', alpha=0.8)\n",
    "plt.scatter(test_targets, y_atoml, color='darkorange',\n",
    "         label='atoml', alpha=0.8)\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "# plt.xlim(0, 20)\n",
    "# plt.ylim(-4, 4)\n",
    "plt.title('scikit-learn vs AtoML')\n",
    "plt.legend(loc=\"best\",  scatterpoints=1, prop={'size': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Dimensionality <a name=\"feature-dimensionality\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "It is much faster to only optimize a single parameter for all dimensions of the feature space. For the squared exponential kernel, there is a width parameter that defines the balance between local and global influence over the feature space. This defaults to a single dimension in sklearn. In AtoML, the default is to optimize a lengthscale for each feature within the feature space. There are a number of considerations that must be accounted for when deciding the dimensions to optimize hyperparameters for. The following provides a benchmark between the two situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdict = {\n",
    "    'k1': {\n",
    "        'type': 'gaussian', 'width': 1., 'scaling': 1., 'dimension': 'single'},\n",
    "    }\n",
    "\n",
    "stime = time.time()\n",
    "gp = GaussianProcess(train_fp=train_features, train_target=train_targets,\n",
    "                     kernel_dict=kdict, regularization=1e-2,\n",
    "                     optimize_hyperparameters=True, scale_data=True)\n",
    "print('training single: {0:.2f}s'.format(time.time() - stime))\n",
    "\n",
    "pred_single = gp.predict(test_fp=test_features)\n",
    "\n",
    "error = get_error(pred_single['prediction'],\n",
    "                  test_targets)['rmse_average']\n",
    "\n",
    "print('Error from single dimension: {0:.3f}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdict = {\n",
    "    'k1': {\n",
    "        'type': 'gaussian', 'width': 1., 'scaling': 1., 'dimension': 'features'},\n",
    "    }\n",
    "\n",
    "stime = time.time()\n",
    "gp = GaussianProcess(train_fp=train_features, train_target=train_targets,\n",
    "                     kernel_dict=kdict, regularization=1e-2,\n",
    "                     optimize_hyperparameters=True, scale_data=True)\n",
    "print('training features: {0:.2f}s'.format(time.time() - stime))\n",
    "\n",
    "pred_features = gp.predict(test_fp=test_features)\n",
    "\n",
    "error = get_error(pred_features['prediction'],\n",
    "                  test_targets)['rmse_average']\n",
    "\n",
    "print('Error from features dimension: {0:.3f}'.format(error))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(test_targets, pred_single['prediction'], 'o', c='b', alpha=0.5)\n",
    "plt.plot(test_targets, pred_features['prediction'], 'o', c='r', alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
