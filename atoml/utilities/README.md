# Utilities

The folder contains some generally useful scripts.

-   [Acquisition functions](#acquisition-functions)
-   [Cost function](#cost-function)
-   [Data setup](#data-setup)
-   [Database functions](#database-functions)
-   [Data cleaning](#data-cleaning)
-   [General utilities](#general-utilities)
-   [Hyperparameter scaling](#hyperparameter-scaling)

## Acquisition functions
[(Back to top)](#utilities)

The cumulative distribution function can be used as an acquisition function.
This accounts for both the prediction and the uncertainty on that prediction.

## Cost function
[(Back to top)](#utilities)

The implementation of the cost function to return some standard data on the
errors generated by a certain set of predictions. This can be used by any of
the regression functions. Typically the most common metrics to pull out are
the root mean squared errors in `rmse_all` or `rmse_average`.

The full list of cost functions is as follows:

*   Root mean squared error
*   Absolute error
*   Epsilon-insensitive error

The cost function to be employed largely depends on the application, with
different metrics being better suited to different situations. Further to the
averaged errors, a number of percentiles are also returned to give an
indication of error variance.

## Data setup
[(Back to top)](#utilities)

A very basic function for generating a unique test set and non-unique training
datasets. This should only be used for simple testing. Some form of
cross-validation should be used in real applications.

## Database functions
[(Back to top)](#utilities)

A couple of database implementations that can be used to efficiently store
feature space representations. Should generally always use the `FingerprintDB`
function as the other is generally too simplistic.

## Data cleaning
[(Back to top)](#utilities)

Some functions to clean data are available.

*   Remove outliers based on targets.
*   Remove invariant features.
*   Remove features with e.g. NaN.

## General utilities
[(Back to top)](#utilities)

More generally useful functions.

*   A simplified implementation of the learning curve.
*   Jacob's geometry hash method.

## Hyperparameter scaling
[(Back to top)](#utilities)

It is generally a good idea to scale features prior to training the Gaussian
process. This can be performed with a number of different scaling functions.
When the hyperparameters are optimized, the resulting values will relate to
whatever scale the features have been transformed to. It is sometimes
beneficial to be able to get the rescaled parameters.

If the default scaling function is used, it is possible to feed that object
into the hyperparameter scaling function to transform the scale of the
parameters to either match the the original scale or the scale of the
standardized data.
