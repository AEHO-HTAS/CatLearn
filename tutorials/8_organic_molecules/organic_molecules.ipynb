{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generators <a name=\"head\"></a>\n",
    "\n",
    "In this tutorial, we will look at generating features from a database of organic donor-acceptor molecules from the [Computational Materials Repository](https://cmrdb.fysik.dtu.dk/?project=solar). This has been downloaded in the [ase-db](https://wiki.fysik.dtu.dk/ase/ase/db/db.html#module-ase.db) format so first off we load the atoms objects and get a target property. Then we convert the atoms objects into a feature array and test out a couple of different models.\n",
    "\n",
    "This tutorial will give an indication of one way in which it is possible to handle atoms objects of different sizes. In particular, we focus on a feature set that scales with the number of atoms. We pad the feature vectors to a constant size to overcome this problem.\n",
    "\n",
    "## Table of Contents\n",
    "[(Back to top)](#head)\n",
    "\n",
    "-   [Requirements](#requirements)\n",
    "-   [Data Setup](#data-setup)\n",
    "-   [Feature Generation](#feature-generation)\n",
    "-   [Predictions](#predictions)\n",
    "-   [Cross-validation](#cross-validation)\n",
    "\n",
    "## Requirements <a name=\"requirements\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "-   [AtoML](https://gitlab.com/atoml/AtoML)\n",
    "-   [ASE](https://wiki.fysik.dtu.dk/ase/)\n",
    "-   [numpy](http://www.numpy.org/)\n",
    "-   [matplotlib](https://matplotlib.org/)\n",
    "-   [seaborn](http://seaborn.pydata.org/index.html)\n",
    "\n",
    "## Data Setup <a name=\"data-setup\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "First, we need to import some functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import ase.db\n",
    "\n",
    "from atoml.fingerprint import StandardFingerprintGenerator\n",
    "from atoml.fingerprint.setup import return_fpv, get_atomic_types\n",
    "from atoml.regression import RidgeRegression, GaussianProcess\n",
    "from atoml.cross_validation import Hierarchy\n",
    "from atoml.utilities.cost_function import get_error\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to extract the atoms objects from the db, as this is the format that AtoML will require to generate the feature vectors. At this point, the target values are also compiled. For this tutorial, we will use the ground state energies as targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect the ase-db.\n",
    "db = ase.db.connect('../../data/solar.db')\n",
    "\n",
    "# Compile a list of atoms and target values.\n",
    "alist = []\n",
    "targets = []\n",
    "for row in db.select():\n",
    "    alist.append(row.toatoms())\n",
    "    targets.append(row.Energy)\n",
    "\n",
    "# Analyze the size of molecules in the db.\n",
    "print('pulled {} molecules from db'.format(len(alist)))\n",
    "size = []\n",
    "for a in alist:\n",
    "    size.append(len(a))\n",
    "\n",
    "print('min: {0}, mean: {1:.0f}, max: {2} molecule size'.format(\n",
    "    min(size), sum(size)/len(size), max(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are a total of ~5400 molecules in the db ranging from 26 to 294 atoms.\n",
    "\n",
    "## Feature Generation <a name=\"feature-generation\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "It can be necessary to work out the full range of elements that need to be accounted for in the model. The feature generator tries to work out the range of elements to account for based on the maximum composition. However, explicitly specifying this is more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atypes = get_atomic_types(train_candidates=alist, test_candidates=None)\n",
    "print('Atom numbers present in data: {}'.format(atypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate the feature array for all the atoms objects. The `return_fpv()` function takes the list of atoms objects and the type of features to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpv = StandardFingerprintGenerator(atom_types=atypes)\n",
    "features = return_fpv(alist, [fpv.eigenspectrum_fpv,\n",
    "                              fpv.composition_fpv])\n",
    "\n",
    "print('{} shape feature matrix'.format(np.shape(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can analyze the distribution of the feature sets. In the following, we see a large number of features in the latter half of the vectors tend to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = np.max(features, axis=0) - np.min(features, axis=0)\n",
    "np.place(dif, dif == 0., [1.])\n",
    "scaled = features.copy() / dif\n",
    "cmap = sns.diverging_palette(250, 15, s=75, l=40, n=1000, center=\"dark\")\n",
    "sns.heatmap(scaled, cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shuffle the data and split the features and targets into a single testing and training dataset for our initial studies. To start with we can take approximately a fifth of the data to train on (1000 data points), and test on the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the order of the data.\n",
    "targets = np.reshape(targets, (len(targets), 1))\n",
    "data = np.concatenate((features, targets), axis=1)\n",
    "np.random.shuffle(data)\n",
    "features = data[:, :-1]\n",
    "targets = np.reshape(data[:, -1:], (np.shape(data)[0],))\n",
    "\n",
    "# Divide up the data into a test and training set.\n",
    "train_size = 1000\n",
    "train_features = features[:train_size, :]\n",
    "test_features = features[train_size:, :]\n",
    "train_targets = targets[:train_size]\n",
    "test_targets = targets[train_size:]\n",
    "\n",
    "print('training feature size: {0}, training target size: {1}'.format(\n",
    "    np.shape(train_features), np.shape(train_targets)))\n",
    "print('testing feature size: {0}, testing target size: {1}'.format(\n",
    "    np.shape(test_features), np.shape(test_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions <a name=\"predictions\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "We can now try predictions with ridge regression to start. This clearly performs very well with this data. Based on these results, it is unlikely that you would consider moving to more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the ridge regression function.\n",
    "rr = RidgeRegression(W2=None, Vh=None, cv='loocv')\n",
    "b = rr.find_optimal_regularization(X=train_features, Y=train_targets)\n",
    "coef = rr.RR(X=train_features, Y=train_targets, omega2=b)[0]\n",
    "\n",
    "# Test the model.\n",
    "sumd = 0.\n",
    "err = []\n",
    "pred = []\n",
    "for tf, tt in zip(test_features, test_targets):\n",
    "    p = np.dot(coef, tf)\n",
    "    pred.append(p)\n",
    "    sumd += (p - tt) ** 2\n",
    "    e = ((p - tt) ** 2) ** 0.5\n",
    "    err.append(e)\n",
    "error = (sumd / len(test_features)) ** 0.5\n",
    "\n",
    "print(error)\n",
    "\n",
    "plt.plot(test_targets, pred, 'o', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for the purposes of this tutorial, we then train a Gaussian processes regression model to test. In this case, we set up a kernel dictionary that has both the squared exponential and linear kernels. The initial parameters defined in the kernel aren't so important at this stage as they are all optimized when the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdict = {\n",
    "    'k1': {\n",
    "        'type': 'gaussian', 'width': 1., 'scaling': 1., 'dimension': 'single'},\n",
    "    'k2' : {\n",
    "        'type': 'linear', 'scaling': 1.},\n",
    "    }\n",
    "gp = GaussianProcess(train_fp=train_features, train_target=train_targets,\n",
    "                     kernel_dict=kdict, regularization=1e-2,\n",
    "                     optimize_hyperparameters=True, scale_data=True)\n",
    "\n",
    "pred = gp.predict(test_fp=test_features)\n",
    "\n",
    "error = get_error(pred['prediction'],\n",
    "                  test_targets)['rmse_average']\n",
    "\n",
    "print(error)\n",
    "\n",
    "plt.plot(test_targets, pred['prediction'], 'o', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the Gaussian process performs slightly worse than the simple ridge regression model. This is to be expected when we are trying to model linear data with a non-linear model. However, the inclusion of the linear kernel results in a good prediction error. If the squared exponential kernel were to be removed from the above example, the resulting model would be the same as the ridge regression model, just trained with the Gaussian process.\n",
    "\n",
    "## Cross-validation <a name=\"cross-validation\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "We can use the hierarchy cross-validation module to investigate how the model performs with different data sizes. In the following, we set up a prediction function. As the ridge regression function performs well, we just redefine this. The prediction function should take in the training and testing data and return a dictionary in the form `{'result': list, 'size': list}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_predict(train_features, train_targets, test_features, test_targets):\n",
    "    \"\"\"Function to perform the RR predictions.\"\"\"\n",
    "    data = {}\n",
    "\n",
    "    # Set up the ridge regression function.\n",
    "    rr = RidgeRegression(W2=None, Vh=None, cv='loocv')\n",
    "    b = rr.find_optimal_regularization(X=train_features, Y=train_targets)\n",
    "    coef = rr.RR(X=train_features, Y=train_targets, omega2=b)[0]\n",
    "\n",
    "    # Test the model.\n",
    "    sumd = 0.\n",
    "    err = []\n",
    "    for tf, tt in zip(test_features, test_targets):\n",
    "        p = np.dot(coef, tf)\n",
    "        sumd += (p - tt) ** 2\n",
    "        e = ((p - tt) ** 2) ** 0.5\n",
    "        err.append(e)\n",
    "    error = (sumd / len(test_features)) ** 0.5\n",
    "\n",
    "    data['result'] = error\n",
    "    data['size'] = len(train_targets)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run the cv and display the resulting learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hierarchy cv class.\n",
    "hv = Hierarchy(db_name='test.sqlite', file_name='hierarchy')\n",
    "# Convert features and targets to simple db format.\n",
    "hv.todb(features=features, targets=targets)\n",
    "# Split the data into subsets.\n",
    "ind = hv.split_index(min_split=100, max_split=2000)\n",
    "\n",
    "# Make the predictions for each subset.\n",
    "pred = hv.split_predict(index_split=ind, predict=rr_predict)\n",
    "\n",
    "# Get mean error at each data size.\n",
    "means, meane = hv.transform_output(pred)\n",
    "\n",
    "# Plot the results.\n",
    "plt.plot(pred[1], pred[0], 'o', c='b', alpha=0.5)\n",
    "plt.plot(means, meane, '-', alpha=0.9, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('hierarchy.pickle')\n",
    "os.remove('test.sqlite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
